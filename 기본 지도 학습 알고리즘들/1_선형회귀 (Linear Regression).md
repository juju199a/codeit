# 1_선형 회귀 (Linear Regression)
## 01. 선형 회귀
1. 선형회귀 (Linear Regression)
    - 머신 러닝: 지도 학습(O), 비지도 학습, 강화 학습
    - 분류, 회귀(O): 연속적인 값을 예측
    - 목표 변수: 맞추려고 하는 값 (target variable / output variable)
    - 입력 변수: 맞추는데 사용하는 값 (input variable / feature)
    - 가설 함수: 우리가 시도하는 이 함수 하나하나를 **가설함수** 라고 한다. (hypothesis function)
    - 최적선: line of best fit
    - 어떤 가설 함수가 얼마나 좋은지 알려면 평가 기준이 있어야 한다.

2. 평균 제곱 오차 (**MSE**)
    - 가설 함수 평가법: 평균 제곱 오차 (MSE, Mean Squared Error)
    - 평균 제곱 오차가 크면: 가설 함수가 데이터에 잘 안맞는다.
    - **평균 제곱 오차가 작으면: 가설 함수가 데이터에 잘 맞다.**

3. 손실 함수 (Loss Function)
    - 또는 비용 함수 (Cost Function)라고도 한다.
    - 가설 함수의 성능을 평가하는 함수
    - **손실 함수가 작으면: 가설 함수가 데이터에 잘 맞다.**
    - 손실 함수가 크면: 가설 함수가 데이터에 잘 안 맞다.
    - 우리가 바꾸는 값들은 X가 아니레 θ(세타)이다.
    - θ(세타) 값들을 바꿔 손실 함수의 아웃풋을 최소화

4. 경사 하강법 (Grandient Descent)
    - 손실함수의 극소점으로 가는 것이 목표이다.
    - **학습률** 알파는 경사를 타고 내려갈 때, 얼마나 많이 움직일지 그 정도를 나타내는 수치입니다.
    - 학습률이 크면 한번에 많이 움직이는 것이고,
    - 학습률이 작으면 조금씩 움직이는 거죠.
    - 보통 학습률은 0.1 이다.
    - 세타0 = 세터0 - (학습률 * 세터0에 대한 편미분)
    - 가설 함수의 최적화
    - 가설 함수 = 세터0 + 세터1*입력값
    - 세타0 = y절편
    - 세터1 = 기울기

5. 모델 평가하기
    - 우리는 계속해서 세터값을 조율하면서 더 좋은 가설 함수를 찾으려고 한다.
    - 이러한 가설 함수는 세상에서 일어나는 상황을 수학적으로 표현한다는 의미에서 모델이라고 부릅니다.
    - 마네킹이나 표현하려고 하는 것을 모델이라고 한다.
    - 모델을 개선하는 것을 **모델을 학습시킨다** 라고 한다.
    - 평균 제곱근 오차 (Root mean square error **(RMSE)**)
    - 루트를 왜 하는 거냐면, 집 가격을 예측한다고 하면, 목표 변수의 단위는 원이다.
    - 그런데, 오차 제곱을 하면, 원 제곱이 됩니다. 
    - 와 닿지 않기 때문에, 루트를 씌운다.
    - 참고로, **오차에 제곱을 하는 이유는** 
        - 양수, 음수를 없애 주기 위해서 이다.
        - 큰 오차에 큰 가중치를 주기 위해서이다.
        - 미분을 통해 최적화를 쉽게 구행할 수 있다.
        - 정규 분포를 따른다고 하면, 모델 추정치에 더 정확하게 근접할 수 있다.
    - 학습과 평가를 위한 데이터를 나눈다!
    - 학습 데이터 training set
    - 평가 데이터 test set > **RMSE 를 구한다.**

6. sklearn
    - Python에는 사이킷 런 (Scikit-learn) 이라는 것이 있다.
'''
x_train, x_test, y_train, y_test = train_test_split(x,y, test_size=0.2, random_state=5)
model = LinearRegression()
model.fit(x_train, y_train)
y_test_predict = model.predict(x_test)

mse = mean_squared_error(y_test, y_test_predict)
mse ** 0.5
'''


 6. 로지스틱 회귀 ()   
    - (참고) 시그모이드 함수(확률 예측)  vs 로지스틱 성장 곡선(시간에 따른 성장)


