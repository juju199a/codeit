1. 퍼셉트론 (Perceptron)
    - 인공 신경망의 가장 기본적인 형태
    - 입력 데이터 * 가중치, 노드, 출력값

2. 이진 분류 문제
    - 초기 퍼셉트론은 이진 분류 문제를 해결하기 위해서 사용하였다.
    - 입력 데이터를 두개의 클래스로 분류하는 것을 말한다.

3. 이진 분류를 위한 퍼셉트론의 구조
    - 입력값 * 가중치 - 특정 임곗값 이상 -> 출력 1
    - 입력값 * 가중치 - 특정 임곗값 미만 -> 출력 0
    - 계단 함수

4. 임곗값
    - 임곗값 (어떤 기준값)
    - 임곗값 0 이상일 때는 1을 출력
    - 임곗값 0 이하일 때는 0을 출력 하는 방식
    - 인공 신경망에서 활성화 함수로 사용되었다.

5. 편향
    - 여기서 편향이라고 불리는 바이어스 값 b 는 퍼셉트론의 각 노드에 추가되어 있는 항을 말합니다.

6. 단층 퍼셉트론
    - 입력층 -> 출력층
    - 입력층: 외부에서 데이터 수용
    - 출력층: 최종 결과 생성 후 외부 전달

7. 출력층의 노드의 개수는 1개 일수도, 여러개 일 수도 있습니다.
    - 딥 런닝 모델에서 출력 노드의 개수는 모델이 해결하고자 하는 문제의 유형에 따라 결정 되는데요.
    - 주로 이진 분류나 회귀 문제에 대해서는 하나의 출력 노드를 사용하고
    - 두개 이상의 클래스를 분류하는 다중 분류 문제에서는 여러개의 출력 노드를 사용합니다.
    - 이 두층으로 이뤄진 단층 퍼셉트론은 간단한 데이터 패턴을 처리하기에는 유리하지만,
    - 복잡한 데이터 패턴의 학습을 하기에는 많은 층을 포함한 다층 퍼셉트론이 필요할 수 있습니다.

8. 단층 퍼셉트론
    - 단층 퍼셉트론은 선형 경계를 설정하는데 특화된 간단한 모델입니다.
    - 선형 경계를 통해 특정 클래스를 분류하죠.
    - 대부분이 선형으로 분류 되지 않는다.
    - 비선형 경계를 이뤄야지 분류가 된다.

9. 다층 퍼셉트론 (Multi-Layer Perceptron)
    - MLP

10. 레이어 (Layer)
    - 여러 노드들이 모인 집합
    - 신경망의 구조를 구성하는 단위
    - 입력층
    - 은닉층
    - 출력층

11. 은닉층
    - 필요에 따라 여러기 추가
    - 은닉층을 여러개 가진 깊은 모듈의 신경망을 심층 신경망(Depep Neural Network)라고 합니다.
    - 입력층과 출력층 사이에 위치
    - 복잡한 특성이나 패턴을 추출하고 학습
    - 비선형적 데이터의 특성 학습 가능 -> 보다 정교한 모델 구축 가능

12. 순전파 (Forward Propagation)
    - 다층 퍼셉트론에서는 각 층에서 수행된는 가중합 연산을 순전파라는 방식을 통해 진행합니다.
    - 가중합 연산(weighted sum): 각각의 값에 특정 **가중치**를 부여해 더한 합을 의미합니다.
    - 모든 값이 동일한 중요도를 가지지 않는 경우에 사용되며, 특히 머신러닝과 통계에서 자주 확용됩니다.
    - **입력 데이터가 네트워크의 각 층을 통과해 최종 출력을 생성하는 과정**

13. 활성화 함수
    - 이 과정에서 각 층의 뉴런은 활성화 함수를 사용하여 다음층으로의 출력을 생성합니다.

14. 손실 함수 (Loss Function)
    - 결과적으로 출력된 출력값은 예측값으로 사용되며, 이 예측값을 실제값과 비교 하는 손실함수를 통해 네트워크의 성능을 평가하고 최적화합니다.
    - 손실함수는 모델의 예측이 실제 데이터와 얼마나 차이가 나는지를 측정하고 이 정보를 바탕으로 모델의 학습을 조정하는데요.

15. 시그모이드 (비선형 활성화 함수)
    - 다층 퍼셉트론에서는 은닉층을 여러기 쌓는 것 뿐만 아니라 시그모이드와 같은 비선형 활성화 함수를 사용한다는 특징이 있는데요. 
    - 사실 은닉층을 여러개 쌓아도 활성화 함수가 선형이면 비선형 연산은 불가능하다고 볼 수 있다.
    - 그래서 각 층마다 비선형 함수를 적용해서, 좀 더 복잡한 데이터를 효과적으로 학습할 수 있도록 하는 것입니다.

16. 다양한 비선형 함수 (각기 다른 특성)
    - Sigmoid
    - tanh
    - ReLU
    - Leaky ReLU

17. 역전파
    - 딥러닝 모델이 잘 학습되었다는 것은 예측 결과값이 실제값에 가까워지며, 오차가 최소화된 상태를 의미하는데요.
    - 이런 상황에서 우리의 목표는 손실 함수의 값을 가능한한 최소로 줄이는 것입니다.
    - 그러나 초기 가중치의 값이 랜덤하게 설정되기 때문에, 예측값과 실제값 사이에 상당한 오차가 발생
    - 이 오차를 줄이기 위해 필요한 것이 **역전파**이다.
    - 역전파는 출력에서 발생한 오차를 신경망을 거슬러 올라가며 **각 층의 가중치가 오차에 얼마나 기여하는지 계산하고, 이 정보를 사용하여 가중치를 업데이트 합니다.**
    - 이 과정을 통해 신경망은 오차를 최소화 하는 방향으로 점차 개선 됩니다.

18. 역전파 (경사하강법)
    - 역전파 과정에서는 경사하강법을 통해 **손실함수를 최소화 하는 방향**으로 가중치 조정이 이뤄집니다.
    - 경사하강법은 손실함수의 **그레이디언트**. 즉 기울기를 계산하고, 이 그레이디언트의 반대 방향으로 조금씩 이동함으로써 함수의 최소값을 찾아가는 최적화 기법인데요.
    - 이 과정이 반복되며, 점차 최저값에 도달하게 됩니다.

19. 역전파 (손실함수의 최솟값)
    - 초기 가중치 랜덤 설정
    - 기울기 계산을 통해 어떻게 최솟값으로 움직일지 결정
    - 위 과정 반복 후 최적값 도달: 손실을 최소화하는 가중치 값 도출

20. **손실을 최소화 하는 가중치 값 도출** 수식
    - W(new) = W(old) - 알파*Gradient
    - W(old): 이전의 가중치
    - -알파*Gradient : 기울기(Gradient)의 반대방향으로 업데이트
    - W(new): 새로운 가중치
    - 알파: 학습률(Learning Rate)

21. 학습률
    - 각 반복에서 가중치가 얼마나 조정될지 걸정
    - 너무 높으면 손실(Loss)값 발산 우려
    - 너무 낮으면 학습 속도 저하 우려

22. 출력함수
    - 마지막 층
    - 회귀나 분류와 같은 문제를 해결할 때, 이 출력함수가 필수적으로 사용됩니다.

23. 회귀 문제에서의 출력함수
    - 회귀 문제를 해결할 때는 보통 신경망의 마지막 출력값을 그대로 사용
    - y = wx + b
    - y: 출력값 (연속적인 실수값을 출력한다.)

24. 분류 문제
    - 분류에서는 다른 함수가 필요합니다.
    - 이진 분류: 2개의 클래스 중 하나를 선택하는 문제
    - 다중 분류: 3개 이상의 클래스 중에서 하나를 선택해야 하는 문제

25. 이진 분류
    - 이진 분류에서는 **시그모이드함수**를 사용하여 신경망의 출력을 0과 1사이의 값으로 변환합니다.
    - 이 함수를 통해 출력된 값은 해당 클래스에 속할 확률로 해석할 수 있습니다.
    - z = wx + b
    - z는 입력 x와 가중치 w의 선형 조합이라고 이해 하면 됩니다.

26. 다중 분류
    - 다중 분류 문제에서는 출력함수로 **소프트맥스** 함수를 사용합니다.
    - 소프트맥스 함수는 각 클래스에 대한 출력을 확률분포로 변환하여 
    - 각 클래스에 속할 확률로 해석
    - 시그모이드와 마찬가지로 **0과 1사이의 값**이 나옵니다.
    - z(i) = w(i)*w + b(i)
    - z(i)는 모델의 i 클래스에 대한 입력 x와 가중치 w에 대한 선형조합이라고 할 수 있어요.
    - 출력노드 3개(고양이, 개, 새)
    - [0.1,0.7,0.2]

27. 손실 함수(Loss Function)
    - 모델이 실제값을 얼마나 잘 예측하는지를 측정
    - 모델의 예측이 실제값과 얼마나 차이가 나는지를 나타냅니다.
    - 손실값이 크다는 것은 모델이 데이터를 잘 표현하고 있지 못하고 있다는 뜻이에요.
    - 따라서 우리는 손실 함수의 값을 가능한 작게 만들어야 하고 이것이 전반적인 딥러닝 학습의 목표라고 할 수 있습니다.
    - 회귀 문제인지 분류 문제인지에 따라서 사용하는 손실 함수가 다르다.

28. 회귀 문제에서의 손실 함수
    - 평균 제곱 오차 (Mean Squared Error, MSE)
    - 모든 데이터의 포인트에 대해 예측값과 실제값의 차이를 구해서 이 값들을 제곱하고, 모든 샘플에 대해 합산한 후 전체 데이터 수 N으로 나눠 평균을 구하면 됩니다.
    - 여기서 제곱을 하면 양수로 바꿔 주는 효과가 있다.
    - 제곱을 해 주면, 큰 오차에 대해 더 큰 페널티를 부여하여 모델이 큰 오차를 내는 것을 피하도록 하는 유도하는 효과도 있습니다.

29. 분류 문제에서의 손실 함수
    - 분류 문제에서는 **이진 교차 엔트로피**와 **교차 엔트로피**를 주로 사용합니다.
    - 이들 중 어떤 손실 교차 엔트로피를 선택할 지는 분류 문제가 이진 분류 문제인지 다중 분류 문제인지 따라 결정됩니다.
    - 두 유형의 손실 함수는 모델이 불확실성을 가질 때 더 큰 손실을 부과합니다.
    - 확실히 잘못된 예측에 대해서는 특히 큰 패널티를 주고요.
    - 이를 통해 모델이 더 정확한 예측을 하도록 유도한다.

30. 이진 교차 엔트로피 (Binary Cross-Entropy)
    - 이진 분류 문제에 사용되며, 모델이 예측한 확률이 실제 레이블과 얼마나 가까운지를 측정합니다.
    - y(i)는 0 또는 1을 나타내는 실제 레이블값, ^y(i)는 모델이 예측한 확률
    - 첫번째 항은 실제 레이블이 1일 때, 모델의 예측 확률이 1에 가까워야 함을 의미하고,
    - 두번째 항은 실제 레이블이 0일 때, 모델의 예측 확률이 0에 가까워야 함을 의미한다고 보면 된다.

31. 교차 엔트로피 (Cross-Entropy)
    - 다중 분류 문제에서는 교차 엔트로피를 사용한다.
    - 모델은 여러 클래스 중, 하나에 속할 확률을 예측하고, 모델이 예측한 확률 분포가 실제 레이블의 분포와 얼마나 일치 하는지를 평가합니다.
    - C는 클래스의 수 
    - y(i,c)는 i 번째 샘플이 c클래스에 속하는 경우를 원핫인코딩으로 나타낸 백터를 나타낸다.
    - i 번째 샘플이 C클래스에 속할 예측확률
    - 교차 엔트로피 함수는 각 샘플에 대해 실제 레이블에 해당하는 확률에 로그값을 취하고 
    - 이를 모든 걸쳐 합산한 후, 전체 샘플에 대해 평균을 내는 방식으로 작동합니다.
    - 결과적으로 모델이 불확실성을 덜 가지고, 정확한 예측을 할 수 있도록 돕는 효과가 있다.



